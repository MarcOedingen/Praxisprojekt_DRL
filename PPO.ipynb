{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PPO.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdj6g1ijOcwe"
      },
      "source": [
        "pip install stable-baselines3[extra]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XP8Q751pOita"
      },
      "source": [
        "%load_ext tensorboard\n",
        "\n",
        "import gym\n",
        "import torch as th\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.vec_env import VecNormalize, VecMonitor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCRdj3QoOjqC"
      },
      "source": [
        "log_name = \"PPO_CartPole-v1_TensorBoard\"\n",
        "env = make_vec_env(\"CartPole-v1\",n_envs=8)\n",
        "model = PPO(\"MlpPolicy\", env, n_steps=128, batch_size=256, gae_lambda= 0.8, gamma=0.98, n_epochs=20, ent_coef=0.0, learning_rate = 0.001, clip_range=0.001, verbose=1, tensorboard_log=f\"./{log_name}/\")\n",
        "model.learn(total_timesteps=100000)\n",
        "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=100)\n",
        "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")\n",
        "%tensorboard --logdir /content/PPO_CartPole-v1_TensorBoard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wf3jcSHW8tEz"
      },
      "source": [
        "log_name = \"PPO_MountainCar-v0_TensorBoard\"\n",
        "env = make_vec_env(\"MountainCar-v0\",n_envs=16)\n",
        "env = VecNormalize(env, norm_obs=True, norm_reward=True)\n",
        "\n",
        "policy_kwargs = dict(activation_fn=th.nn.Tanh,\n",
        "                     net_arch=[dict(pi=[64, 64], vf=[64, 64])])\n",
        "venv = VecMonitor(venv=env)\n",
        "model = PPO('MlpPolicy', venv, policy_kwargs=policy_kwargs, batch_size=256, n_steps=128, gamma=0.99, learning_rate=0.001, ent_coef=0.001,\n",
        "            clip_range=0.1, n_epochs=50, gae_lambda=0.97, max_grad_norm=0.9, vf_coef=0.2, verbose=1, tensorboard_log=f\"./{log_name}/\")\n",
        "model.learn(total_timesteps=500000)\n",
        "mean_reward, std_reward = evaluate_policy(model, venv, n_eval_episodes=100)\n",
        "print(f\"mean_reward:{VecNormalize.unnormalize_reward(venv,mean_reward):.2f} +/- {VecNormalize.unnormalize_reward(venv,std_reward):.2f}\")\n",
        "\n",
        "%tensorboard --logdir /content/PPO_MountainCar-v0_TensorBoard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4rj_0o_VKRB"
      },
      "source": [
        "log_name = \"PPO_MountainCarContinuous-v0_TensorBoard\"\n",
        "env = make_vec_env(\"MountainCarContinuous-v0\",n_envs=1)\n",
        "env = VecNormalize(env,norm_reward=True,norm_obs=True)\n",
        "\n",
        "venv = VecMonitor(venv=env)\n",
        "policy_kwargs = dict(log_std_init=-3.29, ortho_init=False)\n",
        "model = PPO('MlpPolicy', env, n_steps=8, gamma=0.999, ent_coef=0.00429, use_sde=True, vf_coef=0.19,\n",
        "            policy_kwargs=policy_kwargs, batch_size=256, learning_rate=7.77e-05, clip_range=0.1,\n",
        "            n_epochs=10, gae_lambda=0.9, max_grad_norm=5,verbose=1, tensorboard_log=f\"./{log_name}/\")\n",
        "model.learn(total_timesteps=20000)\n",
        "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=100)\n",
        "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")\n",
        "\n",
        "%tensorboard --logdir /content/PPO_MountainCarContinuous-v0_TensorBoard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVoZwSKIDwar"
      },
      "source": [
        "log_name = \"PPO_Acrobot-v1_TensorBoard\"\n",
        "env = make_vec_env(\"Acrobot-v1\",n_envs=16)\n",
        "env = VecNormalize(env, norm_obs=True, norm_reward=True)\n",
        "policy_kwargs = dict(activation_fn=th.nn.Tanh,\n",
        "                     net_arch=[dict(pi=[64, 64], vf=[64, 64])])\n",
        "\n",
        "venv = VecMonitor(venv=env)\n",
        "model = PPO('MlpPolicy', env, policy_kwargs=policy_kwargs, n_steps=8, gamma=0.999, batch_size=512, learning_rate=0.00015312200830971642, ent_coef=1.777074275127635e-05,\n",
        "            clip_range=0.3, n_epochs=20, gae_lambda=0.98, max_grad_norm=1, vf_coef=0.3448244058995513,  verbose=1, tensorboard_log=f\"./{log_name}/\")\n",
        "model.learn(total_timesteps=50000)\n",
        "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=100)\n",
        "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")\n",
        "\n",
        "%tensorboard --logdir /content/PPO_Acrobot-v1_TensorBoard"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}