{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A2C.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xn2tRmyHKyRz"
      },
      "source": [
        "pip install stable-baselines3[extra]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F201cLg6K2Nl"
      },
      "source": [
        "%load_ext tensorboard\n",
        "\n",
        "import gym\n",
        "import torch as th\n",
        "from stable_baselines3 import A2C\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.vec_env import VecNormalize, VecMonitor\n",
        "from typing import Callable"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUW0YSmRXctL"
      },
      "source": [
        "def linear_schedule(initial_value: float) -> Callable[[float], float]:\n",
        "    def func(progress_remaining: float) -> float:\n",
        "        return progress_remaining * initial_value\n",
        "    return func"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJrL3uMbe0zS"
      },
      "source": [
        "log_name = \"A2C_CartPole-v1_TensorBoard\"\n",
        "env = make_vec_env(\"CartPole-v1\",n_envs=8)\n",
        "policy_kwargs = dict(ortho_init=False, activation_fn=th.nn.ReLU,\n",
        "                     net_arch=[dict(pi=[128, 128], vf=[128, 128])])\n",
        "\n",
        "model = A2C(\"MlpPolicy\", env, policy_kwargs=policy_kwargs, gamma=0.999, normalize_advantage=True, max_grad_norm=0.6, use_rms_prop=True, gae_lambda=0.98, n_steps=64, learning_rate=linear_schedule(0.003479714650024201),\n",
        "            ent_coef=1.2071544125414278e-06, vf_coef=0.15287763485632272, verbose=1, tensorboard_log=f\"./{log_name}/\")\n",
        "model.learn(total_timesteps=500000)\n",
        "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=100)\n",
        "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")\n",
        "%tensorboard --logdir /content/A2C_CartPole-v1_TensorBoard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFi8e281FXV8"
      },
      "source": [
        "log_name = \"A2C_MountainCar-v0_TensorBoard\"\n",
        "env = make_vec_env(\"MountainCar-v0\",n_envs=16)\n",
        "env = VecNormalize(env,norm_reward=True,norm_obs=True)\n",
        "policy_kwargs = dict(ortho_init=False, activation_fn=th.nn.Tanh,\n",
        "                     net_arch=[dict(pi=[64, 64], vf=[64, 64])])\n",
        "\n",
        "venv = VecMonitor(venv=env)\n",
        "model = A2C(\"MlpPolicy\", env, policy_kwargs=policy_kwargs, verbose=1, gamma=0.999, gae_lambda=0.8, n_steps=32, learning_rate=linear_schedule(0.009446314774919641),\n",
        "            ent_coef=1.76756270433399e-07, vf_coef=0.12457053994527265, max_grad_norm=0.6, normalize_advantage=False, use_rms_prop=True, tensorboard_log=f\"./{log_name}/\")\n",
        "model.learn(total_timesteps=1000000)\n",
        "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=100)\n",
        "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")\n",
        "%tensorboard --logdir /content/A2C_MountainCar-v0_TensorBoard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLBIskN2WKdI"
      },
      "source": [
        "log_name = \"A2C_MountainCarContinuous-v0_TensorBoard\"\n",
        "env = make_vec_env(\"MountainCarContinuous-v0\",n_envs=4)\n",
        "env = VecNormalize(env,norm_reward=True,norm_obs=True)\n",
        "policy_kwargs = dict(activation_fn=th.nn.Tanh,\n",
        "                     net_arch=[dict(pi=[64, 64], vf=[64, 64])])\n",
        "\n",
        "venv = VecMonitor(venv=env)\n",
        "model = A2C(\"MlpPolicy\", env, policy_kwargs=policy_kwargs,use_rms_prop=True, verbose=1, tensorboard_log=f\"./{log_name}/\")\n",
        "model.learn(total_timesteps=150000)\n",
        "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=100)\n",
        "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")\n",
        "%tensorboard --logdir /content/A2C_MountainCarContinuous-v0_TensorBoard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-dth6tK0UgX"
      },
      "source": [
        "log_name = \"A2C_Acrobot-v1_TensorBoard\"\n",
        "env = make_vec_env(\"Acrobot-v1\",n_envs=16)\n",
        "env = VecNormalize(env,norm_reward=True,norm_obs=True)\n",
        "policy_kwargs = dict(ortho_init=True,activation_fn=th.nn.Tanh,\n",
        "                     net_arch=[dict(pi=[128, 128], vf=[128, 128])])\n",
        "\n",
        "venv = VecMonitor(venv=env)\n",
        "model = A2C(\"MlpPolicy\", env, policy_kwargs=policy_kwargs, use_rms_prop=True, n_steps=16, max_grad_norm=2, verbose=1, tensorboard_log=f\"./{log_name}/\")\n",
        "model.learn(total_timesteps=500000)\n",
        "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=100)\n",
        "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")\n",
        "%tensorboard --logdir /content/A2C_Acrobot-v1_TensorBoard"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}